{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qK_lPvRlBdMB"
   },
   "source": [
    "# LearnAI - Introduction to Modern NLP with and CORD-19\n",
    "\n",
    "**IMPORTANT: Make sure that you have GPU set as your Hardware Accelerator in `Runtime > Change runtime type` before running this Colab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBitmOTKLLh6"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-akt8p7SLLh5"
   },
   "source": [
    "### Install HuggingFace's Transfomers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ccxZ495wLLhx",
    "outputId": "13a01757-2c18-499f-de11-0b2c12c35abd"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/transformers\n",
    "\n",
    "import os\n",
    "os.chdir('/content/transformers')\n",
    "os.mkdir('/content/data')\n",
    "!pip install .\n",
    "!pip install -r ./examples/requirements.txt\n",
    "\n",
    "os.chdir('/content/transformers/examples')\n",
    "\n",
    "!pip install dict_to_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3R6Qo0RpLLhf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from dict_to_obj import DictToObj\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4_7gmGDLLhV"
   },
   "source": [
    "### Download CORD-19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1X2D8vP0LLhE",
    "outputId": "4c5ee044-0407-4cd1-dc9d-382572d38e7d"
   },
   "outputs": [],
   "source": [
    "# Download the train and test set.\n",
    "!wget -nc -O /content/data/abstractstest.txt https://raw.githubusercontent.com/PubChimps/CORD-19/master/abstractstest.txt\n",
    "!wget -nc -O /content/data/abstractstrain.txt https://raw.githubusercontent.com/PubChimps/CORD-19/master/abstractstrain3.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TsAPAdsGV_qL"
   },
   "source": [
    "### Begin Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wxSazIvWWK43",
    "outputId": "f2875a10-131c-4cfa-b3ea-6cf975994f31"
   },
   "outputs": [],
   "source": [
    "!python run_language_modeling.py \\\n",
    "    --output_dir='/content/transformers/output' \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --num_train_epochs=1.0 \\\n",
    "    --do_train \\\n",
    "    --train_data_file=/content/data/abstractstrain.txt \\\n",
    "    --per_gpu_train_batch_size=2 \\\n",
    "    --block_size=512 \\\n",
    "    --gradient_accumulation_steps=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VzV8iTrphJl"
   },
   "source": [
    "## **Download and Preprocess CORD-19**\n",
    "CORD-19 is a collection of json spread across 4 different sub-directories, dependending on the paper's license. This section transforms CORD-19 from json to a single string representing all of their abstracts, split into a train and test set. This section will be skipped during LearnAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xOzFhwDSqOg3"
   },
   "source": [
    "### Download and unzip dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "C33GutF1QVEV",
    "outputId": "9c4a81c1-18e3-41f2-b702-a7d3de5aa34f"
   },
   "outputs": [],
   "source": [
    "!wget -nc -O /content/CORD-19.zip https://ibm.box.com/shared/static/m8ualk8ke9bqxtzqz19osmnd5ryqeflu.zip\n",
    "!unzip /content/CORD-19.zip -d /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjCxVTY6cfjQ"
   },
   "source": [
    "### Many jsons -> single string\n",
    "The following functions crawl through the directories of the CORD-19 data set, load each file in the directory, and place it's abstract into Pandas, where it is converted to a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7qpSJHkx8rt"
   },
   "outputs": [],
   "source": [
    "def format_body(body_text):\n",
    "    texts = [(di['section'], di['text']) for di in body_text]\n",
    "    texts_di = {di['section']: \"\" for di in body_text}\n",
    "    \n",
    "    for section, text in texts:\n",
    "        texts_di[section] += text\n",
    "\n",
    "    body = \"\"\n",
    "\n",
    "    for section, text in texts_di.items():\n",
    "        body += section\n",
    "        body += \"\\n\\n\"\n",
    "        body += text\n",
    "        body += \"\\n\\n\"\n",
    "    \n",
    "    return body\n",
    "\n",
    "def load_files(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    raw_files = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        filename = dirname + filename\n",
    "        file = json.load(open(filename, 'rb'))\n",
    "        raw_files.append(file)\n",
    "    \n",
    "    return raw_files\n",
    "\n",
    "def generate_clean_df(all_files):\n",
    "    cleaned_files = []\n",
    "    for file in all_files:\n",
    "        features = [\n",
    "            format_body(file['abstract'])\n",
    "        ]\n",
    "\n",
    "        cleaned_files.append(features)\n",
    "    col_names = ['abstract']\n",
    "    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n",
    "    clean_df.head()\n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3iyElOxyAOA"
   },
   "outputs": [],
   "source": [
    "\n",
    "biomed_dir = '/content/data/biorxiv_medrxiv/biorxiv_medrxiv/'\n",
    "comm_dir = '/content/data/comm_use_subset/comm_use_subset/'\n",
    "custom_dir = '/content/data/custom_license/custom_license/'\n",
    "noncomm_dir = '/content/data/noncomm_use_subset/noncomm_use_subset/'\n",
    "\n",
    "directories = [biomed_dir,comm_dir,custom_dir,noncomm_dir]\n",
    "df = pd.DataFrame()\n",
    "for directory in directories:\n",
    "    files = load_files(directory)\n",
    "    df = df.append(generate_clean_df(files))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1jDg_3nM1-cO",
    "outputId": "1b820dab-5a12-4db3-e5b6-55ad977ec5f1"
   },
   "outputs": [],
   "source": [
    "abstracts = pd.Series(df['abstract']).str.cat(sep=' ')\n",
    "print(len(abstracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZHqepVG2LMG"
   },
   "source": [
    "### Filter non-English Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRWx9bFZdTgn"
   },
   "source": [
    "#### Characters to ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hCRbUIih2OWJ"
   },
   "outputs": [],
   "source": [
    "CHARS = [\n",
    " '¦',\n",
    " '§',\n",
    " '¨',\n",
    " '©',\n",
    " 'ª',\n",
    " '«',\n",
    " '®',\n",
    " '¯',\n",
    " '°',\n",
    " '±',\n",
    " '²',\n",
    " '³',\n",
    " '´',\n",
    " 'µ',\n",
    " '¶',\n",
    " '·',\n",
    " 'º',\n",
    " '»',\n",
    " '¼',\n",
    " '½',\n",
    " '¿',\n",
    " '×',\n",
    " 'Ø',\n",
    " '÷',\n",
    " 'ø',\n",
    " 'Ɵ',\n",
    " 'Ƶ',\n",
    " 'ǁ',\n",
    " 'ǆ',\n",
    " 'Ǉ',\n",
    " 'ǌ',\n",
    " 'ʹ',\n",
    " 'ʼ',\n",
    " 'ˆ',\n",
    " 'ˇ',\n",
    " 'À',\n",
    " 'Á',\n",
    " 'Â',\n",
    " 'Ã',\n",
    " 'Ä',\n",
    " 'Å',\n",
    " 'Ç',\n",
    " 'È',\n",
    " 'É',\n",
    " 'Ê',\n",
    " 'Í',\n",
    " 'Ð',\n",
    " 'Ñ',\n",
    " 'Ò',\n",
    " 'Ó',\n",
    " 'Ô',\n",
    " 'Õ',\n",
    " 'Ö',\n",
    " 'Ú',\n",
    " 'Û',\n",
    " 'Ü',\n",
    " 'Þ',\n",
    " 'ß',\n",
    " 'à',\n",
    " 'á',\n",
    " 'â',\n",
    " 'ã',\n",
    " 'ä',\n",
    " 'å',\n",
    " 'ç',\n",
    " 'è',\n",
    " 'é',\n",
    " 'ê',\n",
    " 'ë',\n",
    " 'ì',\n",
    " 'í',\n",
    " 'î',\n",
    " 'ï',\n",
    " 'ð',\n",
    " 'ñ',\n",
    " 'ò',\n",
    " 'ó',\n",
    " 'ô',\n",
    " 'õ',\n",
    " 'ö',\n",
    " 'ù',\n",
    " 'ú',\n",
    " 'û',\n",
    " 'ü',\n",
    " 'ý',\n",
    " 'þ',\n",
    " 'ÿ',\n",
    " 'ā',\n",
    " 'Ă',\n",
    " 'ą',\n",
    " 'Ć',\n",
    " 'ć',\n",
    " 'Č',\n",
    " 'č',\n",
    " 'ď',\n",
    " 'Đ',\n",
    " 'ē',\n",
    " 'ę',\n",
    " 'Ě',\n",
    " 'ě',\n",
    " 'Ğ',\n",
    " 'Ĩ',\n",
    " 'Į',\n",
    " 'ı',\n",
    " 'ĸ',\n",
    " 'Ĺ',\n",
    " 'ł',\n",
    " 'ń',\n",
    " 'Ň',\n",
    " 'Ō',\n",
    " 'ō',\n",
    " 'Ő',\n",
    " 'ő',\n",
    " 'Ś',\n",
    " 'ś',\n",
    " 'ŝ',\n",
    " 'ş',\n",
    " 'Š',\n",
    " 'š',\n",
    " 'Ŭ',\n",
    " 'ů',\n",
    " 'ŵ',\n",
    " 'Ŷ',\n",
    " 'ź',\n",
    " 'ż',\n",
    " 'Ž',\n",
    " 'ž',\n",
    " 'Ɖ',\n",
    " 'Ƌ',\n",
    " 'ƌ',\n",
    " 'Ɛ',\n",
    " 'ƚ',\n",
    " 'ǎ',\n",
    " 'ǐ',\n",
    " 'ǒ',\n",
    " 'ǔ',\n",
    " 'ǡ',\n",
    " 'ș',\n",
    " 'ɑ',\n",
    " 'ɛ',\n",
    " 'ɣ',\n",
    " 'ʋ',\n",
    " '˘',\n",
    " '˚',\n",
    " '˛',\n",
    " '˝',\n",
    " '́',\n",
    " '̇',\n",
    " '͕',\n",
    " '͖',\n",
    " '͗',\n",
    " '͘',\n",
    " 'ͬ',\n",
    " 'Ͳ',\n",
    " 'а',\n",
    " 'б',\n",
    " 'в',\n",
    " 'г',\n",
    " 'д',\n",
    " 'е',\n",
    " 'ж',\n",
    " 'з',\n",
    " 'и',\n",
    " 'й',\n",
    " 'к',\n",
    " 'л',\n",
    " 'м',\n",
    " 'н',\n",
    " 'о',\n",
    " 'п',\n",
    " 'р',\n",
    " 'с',\n",
    " 'т',\n",
    " 'у',\n",
    " 'ф',\n",
    " 'х',\n",
    " 'ц',\n",
    " 'ч',\n",
    " 'ш',\n",
    " 'щ',\n",
    " 'ы',\n",
    " 'ь',\n",
    " 'э',\n",
    " 'ю',\n",
    " 'я',\n",
    " 'ӧ',\n",
    " 'Յ',\n",
    " 'Ն',\n",
    " '؉',\n",
    " '؊',\n",
    " '؋',\n",
    " '،',\n",
    " '؍',\n",
    " '؎',\n",
    " 'ء',\n",
    " 'آ',\n",
    " 'أ',\n",
    " 'ؤ',\n",
    " 'إ',\n",
    " 'ئ',\n",
    " 'ا',\n",
    " 'ب',\n",
    " 'ة',\n",
    " 'ت',\n",
    " 'ث',\n",
    " 'ج',\n",
    " 'ح',\n",
    " 'خ',\n",
    " 'د',\n",
    " 'ذ',\n",
    " 'ر',\n",
    " 'ز',\n",
    " 'س',\n",
    " 'ش',\n",
    " 'ص',\n",
    " 'ض',\n",
    " 'ط',\n",
    " 'ظ',\n",
    " 'ع',\n",
    " 'غ',\n",
    " 'ف',\n",
    " 'ق',\n",
    " 'ك',\n",
    " 'ل',\n",
    " 'م',\n",
    " 'ن',\n",
    " 'ه',\n",
    " 'و',\n",
    " 'ى',\n",
    " 'ي',\n",
    " 'ً',\n",
    " 'ٌ',\n",
    " 'ٍ',\n",
    " 'َ',\n",
    " 'ُ',\n",
    " 'ِ',\n",
    " 'ّ',\n",
    " 'ْ',\n",
    " 'ܰ',\n",
    " 'ܴ',\n",
    " '݅',\n",
    " '݇',\n",
    " 'ݏ',\n",
    " 'ݑ',\n",
    " 'ݕ',\n",
    " 'ߚ',\n",
    " 'ߜ',\n",
    " 'ߤ',\n",
    " 'ߪ',\n",
    " 'ଝ',\n",
    " 'ଵ',\n",
    " 'ଶ',\n",
    " '᭧',\n",
    " 'ᮊ',\n",
    " 'ᵒ',\n",
    " 'Ḡ',\n",
    " 'ỹ',\n",
    " '‖',\n",
    " '‚',\n",
    " '†',\n",
    " '‡',\n",
    " '•',\n",
    " '…',\n",
    " '‰',\n",
    " '′',\n",
    " '″',\n",
    " '⁄',\n",
    " '⁎',\n",
    " '⁶',\n",
    " '⁹',\n",
    " '₀',\n",
    " '€',\n",
    " '℃',\n",
    " 'ℜ',\n",
    " '™',\n",
    " 'Ω',\n",
    " 'Ⅰ',\n",
    " 'Ⅱ',\n",
    " 'Ⅲ',\n",
    " '→',\n",
    " '↓',\n",
    " '↵',\n",
    " '⇑',\n",
    " '⌬',\n",
    " '⌿',\n",
    " '⍀',\n",
    " '␣',\n",
    " '␤',\n",
    " '␥',\n",
    " '␦',\n",
    " '■',\n",
    " '▪',\n",
    " '▶',\n",
    " '▸',\n",
    " '►',\n",
    " '○',\n",
    " '◗',\n",
    " '★',\n",
    " '☆',\n",
    " '✔',\n",
    " '✜',\n",
    " '✩',\n",
    " '➜',\n",
    " '⩾',\n",
    " '、',\n",
    " '・',\n",
    " 'Ϳ',\n",
    " '΄',\n",
    " '·',\n",
    " 'Ί',\n",
    " 'Α',\n",
    " 'Γ',\n",
    " 'Ε',\n",
    " 'Θ',\n",
    " 'Ι',\n",
    " 'Λ',\n",
    " 'Μ',\n",
    " 'ϩ',\n",
    " 'Ϫ',\n",
    " 'ϫ',\n",
    " 'Ϭ',\n",
    " 'ϭ',\n",
    " 'Ϯ',\n",
    " 'ϯ',\n",
    " 'ϰ',\n",
    " 'ϱ',\n",
    " 'ϲ',\n",
    " 'ϳ',\n",
    " 'ϵ',\n",
    " 'Ϸ',\n",
    " 'Ͻ',\n",
    " 'Ͼ',\n",
    " 'Ј',\n",
    " 'Љ',\n",
    " 'Њ',\n",
    " 'А',\n",
    " 'Б',\n",
    " 'В',\n",
    " 'Д',\n",
    " 'И',\n",
    " 'К',\n",
    " 'Н',\n",
    " 'О',\n",
    " 'Р',\n",
    " 'С',\n",
    " 'Т',\n",
    " 'У',\n",
    " 'Ф',\n",
    " 'Х',\n",
    " 'Ц',\n",
    " 'Ч',\n",
    " 'Ш',\n",
    " '中',\n",
    " '乌',\n",
    " '亏',\n",
    " '代',\n",
    " '何',\n",
    " '充',\n",
    " '冒',\n",
    " '吃',\n",
    " '國',\n",
    " '型',\n",
    " '子',\n",
    " '學',\n",
    " '寄',\n",
    " '寒',\n",
    " '山',\n",
    " '感',\n",
    " '扬',\n",
    " '方',\n",
    " '明',\n",
    " '是',\n",
    " '暑',\n",
    " '替',\n",
    " '板',\n",
    " '根',\n",
    " '桑',\n",
    " '民',\n",
    " '決',\n",
    " '熱',\n",
    " '狗',\n",
    " '理',\n",
    " '生',\n",
    " '福',\n",
    " '脊',\n",
    " '膽',\n",
    " '與',\n",
    " '良',\n",
    " '芳',\n",
    " '藍',\n",
    " '藥',\n",
    " '處',\n",
    " '補',\n",
    " '論',\n",
    " '醫',\n",
    " '钟',\n",
    " '間',\n",
    " '風',\n",
    " '首',\n",
    " '龍',\n",
    " '가',\n",
    " '각',\n",
    " '간',\n",
    " '감',\n",
    " '갑',\n",
    " '강',\n",
    " '같',\n",
    " '개',\n",
    " '객',\n",
    " '거',\n",
    " '걱',\n",
    " '건',\n",
    " '걸',\n",
    " '검',\n",
    " '것',\n",
    " '게',\n",
    " '겨',\n",
    " '격',\n",
    " '겪',\n",
    " '결',\n",
    " '겼',\n",
    " '경',\n",
    " '계',\n",
    " '고',\n",
    " '공',\n",
    " '과',\n",
    " '관',\n",
    " '교',\n",
    " '구',\n",
    " '국',\n",
    " '군',\n",
    " '그',\n",
    " '근',\n",
    " '글',\n",
    " '급',\n",
    " '기',\n",
    " '긴',\n",
    " '길',\n",
    " '까',\n",
    " '꺼',\n",
    " '꼈',\n",
    " '나',\n",
    " '낙',\n",
    " '난',\n",
    " '남',\n",
    " '났',\n",
    " '내',\n",
    " '넷',\n",
    " '년',\n",
    " '노',\n",
    " '높',\n",
    " '누',\n",
    " '느',\n",
    " '는',\n",
    " '능',\n",
    " '니',\n",
    " '다',\n",
    " '단',\n",
    " '달',\n",
    " '당',\n",
    " '대',\n",
    " '던',\n",
    " '도',\n",
    " '동',\n",
    " '되',\n",
    " '된',\n",
    " '두',\n",
    " '드',\n",
    " '든',\n",
    " '들',\n",
    " '등',\n",
    " '따',\n",
    " '때',\n",
    " '또',\n",
    " '라',\n",
    " '람',\n",
    " '램',\n",
    " '략',\n",
    " '량',\n",
    " '러',\n",
    " '렇',\n",
    " '레',\n",
    " '려',\n",
    " '력',\n",
    " '련',\n",
    " '령',\n",
    " '로',\n",
    " '록',\n",
    " '론',\n",
    " '롯',\n",
    " '료',\n",
    " '루',\n",
    " '률',\n",
    " '르',\n",
    " '른',\n",
    " '를',\n",
    " '리',\n",
    " '립',\n",
    " '마',\n",
    " '만',\n",
    " '말',\n",
    " '망',\n",
    " '매',\n",
    " '머',\n",
    " '멀',\n",
    " '메',\n",
    " '며',\n",
    " '면',\n",
    " '명',\n",
    " '모',\n",
    " '목',\n",
    " '못',\n",
    " '무',\n",
    " '문',\n",
    " '물',\n",
    " '미',\n",
    " '밀',\n",
    " '및',\n",
    " '바',\n",
    " '반',\n",
    " '받',\n",
    " '발',\n",
    " '방',\n",
    " '배',\n",
    " '백',\n",
    " '번',\n",
    " '범',\n",
    " '법',\n",
    " '별',\n",
    " '병',\n",
    " '보',\n",
    " '복',\n",
    " '본',\n",
    " '부',\n",
    " '분',\n",
    " '불',\n",
    " '비',\n",
    " '빈',\n",
    " '사',\n",
    " '산',\n",
    " '상',\n",
    " '생',\n",
    " '서',\n",
    " '석',\n",
    " '선',\n",
    " '설',\n",
    " '성',\n",
    " '세',\n",
    " '소',\n",
    " '속',\n",
    " '손',\n",
    " '쇄',\n",
    " '수',\n",
    " '순',\n",
    " '술',\n",
    " '슈',\n",
    " '스',\n",
    " '시',\n",
    " '식',\n",
    " '신',\n",
    " '실',\n",
    " '심',\n",
    " '써',\n",
    " '아',\n",
    " '악',\n",
    " '안',\n",
    " '않',\n",
    " '알',\n",
    " '았',\n",
    " '애',\n",
    " '야',\n",
    " '약',\n",
    " '양',\n",
    " '어',\n",
    " '언',\n",
    " '얼',\n",
    " '없',\n",
    " '었',\n",
    " '에',\n",
    " '여',\n",
    " '역',\n",
    " '연',\n",
    " '염',\n",
    " '였',\n",
    " '영',\n",
    " '예',\n",
    " '와',\n",
    " '왔',\n",
    " '외',\n",
    " '요',\n",
    " '욕',\n",
    " '용',\n",
    " '우',\n",
    " '운',\n",
    " '울',\n",
    " '움',\n",
    " '원',\n",
    " '월',\n",
    " '웠',\n",
    " '위',\n",
    " '유',\n",
    " '육',\n",
    " '율',\n",
    " '으',\n",
    " '은',\n",
    " '을',\n",
    " '음',\n",
    " '응',\n",
    " '의',\n",
    " '이',\n",
    " '인',\n",
    " '일',\n",
    " '임',\n",
    " '입',\n",
    " '있',\n",
    " '자',\n",
    " '작',\n",
    " '잘',\n",
    " '잠',\n",
    " '장',\n",
    " '재',\n",
    " '저',\n",
    " '적',\n",
    " '전',\n",
    " '절',\n",
    " '점',\n",
    " '접',\n",
    " '정',\n",
    " '제',\n",
    " '조',\n",
    " '족',\n",
    " '존',\n",
    " '종',\n",
    " '주',\n",
    " '준',\n",
    " '줄',\n",
    " '중',\n",
    " '증',\n",
    " '지',\n",
    " '직',\n",
    " '진',\n",
    " '질',\n",
    " '징',\n",
    " '차',\n",
    " '착',\n",
    " '찰',\n",
    " '참',\n",
    " '처',\n",
    " '척',\n",
    " '철',\n",
    " '첫',\n",
    " '청',\n",
    " '체',\n",
    " '쳐',\n",
    " '촉',\n",
    " '총',\n",
    " '최',\n",
    " '추',\n",
    " '축',\n",
    " '출',\n",
    " '충',\n",
    " '취',\n",
    " '측',\n",
    " '치',\n",
    " '칠',\n",
    " '코',\n",
    " '콩',\n",
    " '크',\n",
    " '타',\n",
    " '태',\n",
    " '택',\n",
    " '터',\n",
    " '토',\n",
    " '통',\n",
    " '트',\n",
    " '특',\n",
    " '파',\n",
    " '판',\n",
    " '퍼',\n",
    " '편',\n",
    " '평',\n",
    " '폐',\n",
    " '포',\n",
    " '폭',\n",
    " '푛',\n",
    " '표',\n",
    " '품',\n",
    " '프',\n",
    " '피',\n",
    " '하',\n",
    " '학',\n",
    " '한',\n",
    " '할',\n",
    " '함',\n",
    " '항',\n",
    " '해',\n",
    " '핵',\n",
    " '했',\n",
    " '행',\n",
    " '향',\n",
    " '헌',\n",
    " '험',\n",
    " '혀',\n",
    " '현',\n",
    " '형',\n",
    " '호',\n",
    " '혹',\n",
    " '홍',\n",
    " '화',\n",
    " '확',\n",
    " '환',\n",
    " '활',\n",
    " '황',\n",
    " '회',\n",
    " '효',\n",
    " '후',\n",
    " '휴',\n",
    " '흡',\n",
    " '\\u202a',\n",
    " '\\u202b',\n",
    " '\\u202c',\n",
    " '\\ue024',\n",
    " '\\ue02c',\n",
    " '\\ue02e',\n",
    " '\\ue031',\n",
    " '\\ue032',\n",
    " '\\ue033',\n",
    " '\\ue035',\n",
    " '\\ue061',\n",
    " '\\ue062',\n",
    " '\\ue06d',\n",
    " '\\ue152',\n",
    " '\\uf020',\n",
    " '\\uf02b',\n",
    " '\\uf02d',\n",
    " '\\uf02f',\n",
    " '\\uf03d',\n",
    " '\\uf044',\n",
    " '\\uf046',\n",
    " '\\uf05b',\n",
    " '\\uf05d',\n",
    " '\\uf061',\n",
    " '\\uf062',\n",
    " '\\uf063',\n",
    " '\\uf065',\n",
    " '\\uf067',\n",
    " '\\uf06b',\n",
    " '\\uf06c',\n",
    " '\\uf06d',\n",
    " '\\uf09f',\n",
    " '\\uf0a2',\n",
    " '\\uf0a3',\n",
    " '\\uf0a7',\n",
    " '\\uf0ae',\n",
    " '\\uf0b0',\n",
    " '\\uf0b4',\n",
    " '\\uf0b7',\n",
    " '\\uf0bb',\n",
    " '\\uf0d7',\n",
    " '\\uf0e0',\n",
    " '\\uf6d9',\n",
    " '\\uf761',\n",
    " '\\uf762',\n",
    " '\\uf764',\n",
    " '\\uf765',\n",
    " '\\uf766',\n",
    " '\\uf767',\n",
    " '\\uf768',\n",
    " '\\uf769',\n",
    " '\\uf76b',\n",
    " '\\uf76c',\n",
    " '\\uf76e',\n",
    " '\\uf76f',\n",
    " '\\uf770',\n",
    " '\\uf772',\n",
    " '\\uf773',\n",
    " '\\uf774',\n",
    " '\\uf775',\n",
    " '\\uf776',\n",
    " '\\uf777',\n",
    " '\\uf778',\n",
    " '\\uf779',\n",
    " '\\uf77a',\n",
    "'�']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtGYdT25dcLC"
   },
   "source": [
    "#### Filtering String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fU2nafS42Qt6"
   },
   "outputs": [],
   "source": [
    "for c in CHARS:\n",
    "    abstracts = abstracts.replace(c, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e7fBGgKw2woY",
    "outputId": "a14200f4-3b06-4e69-defe-67609616f0b2"
   },
   "outputs": [],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-hqHk_49P6h"
   },
   "outputs": [],
   "source": [
    "abstracts_train = abstracts[:int(len(abstracts)*.9)]\n",
    "abstracts_test = abstracts[int(len(abstracts)*.9):]\n",
    "\n",
    "abstracts_file = open('/content/data/abstractstrain.txt', 'w')\n",
    "n = abstracts_file.write(abstracts_train)\n",
    "abstracts_file.close()\n",
    "\n",
    "abstracts_file = open('/content/data/abstractstest.txt', 'w')\n",
    "n = abstracts_file.write(abstracts_test)\n",
    "abstracts_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUJ2pW0IB4WH"
   },
   "source": [
    "## **Generating abstracts without Transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ocoZ5LgUEHHR",
    "outputId": "cc667459-766c-40f6-aea2-edb81ca29658"
   },
   "outputs": [],
   "source": [
    "with open('/content/data/abstractstrain.txt', 'r') as abstracts_file:\n",
    "     abstractstrain = abstracts_file.read()\n",
    "\n",
    "vocab = sorted(set(abstractstrain))\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in abstractstrain])\n",
    "\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(abstractstrain)//(seq_length+1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "rnn_units = 512\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "EPOCHS=3\n",
    "checkpoint_dir = '/content/tensorflow/output'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfd6zMJKeD4E"
   },
   "source": [
    "### Function to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LdKr9EQtHkj_"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  \n",
    "    num_generate = 500\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "\n",
    "    temperature = .5\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        \n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IoKCkapOeMqs"
   },
   "source": [
    "### Creating new abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "WGpylqC7HqlP",
    "outputId": "aefb9d59-6b31-4f1b-9e46-09c3cf2adc0c"
   },
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=\"Abstract\\n\\nThe corona\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FBza2txbavDh"
   },
   "source": [
    "## Generating abstracts with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MuQnWfK2erqA",
    "outputId": "d0dd9a91-a1b6-4096-f522-61641c78d35e"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained('/content/transformers/output/', pad_token_id=tokenizer.eos_token_id, from_pt=True)\n",
    "\n",
    "input_ids = tokenizer.encode('Abstract\\n\\nCovid-19', return_tensors='tf')\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=150, \n",
    "    top_k=50, \n",
    "    top_p=0.92, \n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "learnai.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
